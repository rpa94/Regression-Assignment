{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Assignment Questions:\n",
        "\n",
        "\n",
        "\n",
        " Que.1  What is Simple Linear Regression?\n",
        "\n",
        " Ans Linear Regression is a commonly used type of predictive analysis. Linear Regression is a statistical approach for modelling the relationship between a dependent variable and a given set of independent variables. It is predicted that a straight line can be used to approximate the relationship. The goal of linear regression is to identify the line that minimizes the discrepancies between the observed data points and the line’s anticipated values.\n",
        "\n",
        "There are two types of linear regression.\n",
        "\n",
        "* Simple Linear Regression\n",
        "* Multiple Linear Regression\n",
        "\n",
        "\n",
        "Simple Linear Regression:\n",
        "\n",
        "In Machine Learning Linear regression is one of the easiest and most popular Machine Learning algorithms.\n",
        "\n",
        "* It is a statistical method that is used for predictive analysis.\n",
        "\n",
        "* Linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc.\n",
        "\n",
        "* Linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it finds how the value of the dependent variable changes according to the value of the independent variable.\n",
        "\n",
        "\n",
        "Que.2 What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "\n",
        "Ans.Linear regression is a statistical method used to understand the relationship between a dependent variable (y) and one or more independent variables (x). However, for the results to be reliable, certain assumptions must be met. Here are the key assumptions of linear regression:\n",
        "\n",
        "* Linear Relationship\n",
        "\n",
        "The first assumption is that there is a linear relationship between the independent variable(s) and the dependent variable. This means that changes in the independent variable(s) should result in proportional changes in the dependent variable. To check this, you can create a scatter plot of the variables. If the points roughly form a straight line, the assumption is met\n",
        "\n",
        "\n",
        "* Independence\n",
        "\n",
        "The residuals (errors) should be independent of each other. This is particularly important in time series data where residuals should not show any pattern over time. You can check this by plotting residuals against time or using the Durbin-Watson test\n",
        "\n",
        "* Homoscedasticity\n",
        "\n",
        "The residuals should have constant variance at every level of the independent variable(s). This is known as homoscedasticity. If the residuals show a pattern or funnel shape when plotted against the fitted values, it indicates heteroscedasticity. You can address this by transforming the dependent variable or using weighted regression\n",
        "\n",
        "* Normality\n",
        "\n",
        "The residuals should be normally distributed. This can be checked using Q-Q plots or statistical tests like the Shapiro-Wilk test. If the residuals do not follow a normal distribution, you can apply transformations to the variables\n",
        "\n",
        "* No Multicollinearity\n",
        "\n",
        "The independent variables should not be highly correlated with each other. Multicollinearity can make it difficult to determine the individual effect of each independent variable. You can check for multicollinearity using scatter plots, correlation matrices, or the Variance Inflation Factor (VIF)\n",
        "\n",
        "* No Endogeneity\n",
        "\n",
        "There should be no relationship between the errors and the independent variables. Endogeneity can lead to biased estimates and incorrect conclusions\n",
        "\n",
        "* Number of Observations\n",
        "\n",
        "The number of observations should be greater than the number of predictors. This ensures that the model has enough data to make accurate predictions\n",
        "\n",
        "\n",
        "* Unique Observations\n",
        "\n",
        "Each observation should be unique and independent of other observations. This ensures that the data accurately represents the population being studied\n",
        "\n",
        "By ensuring these assumptions are met, you can improve the reliability and accuracy of your linear regression model. If any assumptions are violated, there are various methods to address them, such as transforming variables, adding new variables, or using different types of regression models.\n",
        "\n",
        "\n",
        "\n",
        "Que.What does the coefficient m represent in the equation Y=mX+c.\n",
        "\n",
        "Ans  what is m in y=mx+c? In y=mx+c, where where ‘y’ and ‘x’ are the variables and ‘c’ is the y-intercept, ‘m’ represents the gradient, also known as the slope. The slope describes the steepness of the line and is a vital factor in linear regression, optimization problems, and data analysis.\n",
        "\n",
        "\n",
        "Que.4 What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "\n",
        "Ans.The value of c in the equation y = mx + c represents the y-intercept of the line. The intercept is the distance from the origin on the y-axis, where this line cuts the y-axis.\n",
        "\n",
        "\n",
        "Que.5  How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "Ans.The formula for calculating the slope (m) in simple linear regression is m = (n∑x y - (∑x) (∑y)) / (n*******∑x 2 - (∑x) 2)**. The equation of a simple linear regression line (the line of best fit) is y = mx + b. In this equation, Y is the response (dependent) variable, X is the predictor (independent) variable, m is the estimated slope, and b is the estimated intercept. The slope (m) is calculated using the formula above.\n",
        "\n",
        "Que.6  What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "Ans. The least square method is a statistical technique used to find the line of best fit for a set of data points by minimizing the sum of the squares of the residuals (the differences between the observed and predicted values). This method is widely used in regression analysis to predict the behavior of the dependent variable based on the independent variable\n",
        "\n",
        "Formula for Least Square Method\n",
        "\n",
        "To determine the equation of the line of best fit, we use the linear equation: [ y = mx + b ] where:\n",
        "\n",
        "( y ) is the dependent variable,\n",
        "\n",
        "( x ) is the independent variable,\n",
        "\n",
        "( m ) is the slope of the line,\n",
        "\n",
        "( b ) is the y-intercept.\n",
        "\n",
        "The formulas to calculate the slope (( m )) and the y-intercept (( b )) are: [ m = \\frac{n\\sum{xy} - \\sum{x}\\sum{y}}{n\\sum{x^2} - (\\sum{x})^2} ] [ b = \\frac{\\sum{y} - m\\sum{x}}{n} ] where:\n",
        "\n",
        "( n ) is the number of data points,\n",
        "\n",
        "( \\sum{xy} ) is the sum of the product of each pair of ( x ) and ( y ) values,\n",
        "\n",
        "( \\sum{x} ) is the sum of all ( x ) values,\n",
        "\n",
        "( \\sum{y} ) is the sum of all ( y ) values,\n",
        "\n",
        "( \\sum{x^2} ) is the sum of the squares of ( x ) values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Que.7 How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\n",
        "Ans.The coefficient of determination is a number between 0 and 1 that measures how well a statistical model predicts an outcome.\n",
        "\n",
        "Interpreting the coefficient of determination:\n",
        "\n",
        "\n",
        "Coefficient of determination (R2)         \tInterpretation\n",
        "\n",
        "0\t                                     The model does not\n",
        "                                      predict the outcome.\n",
        "\n",
        "Between 0 and 1\t                      The model partially\n",
        "                                     predicts the outcome.\n",
        "\n",
        "1\t                                   The model perfectly\n",
        "                                    predicts the outcome.\n",
        "\n",
        "\n",
        "The coefficient of determination is often written as R2, which is pronounced as “r squared.” For simple linear regressions, a lowercase r is usually used instead (r2).\n",
        "\n",
        "\n",
        "\n",
        "Que.8 What is Multiple Linear Regression\n",
        "\n",
        "Ans. Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of MLR is to model the linear relationship between the explanatory (independent) variables and response (dependent) variables. In essence, multiple regression is the extension of ordinary least-squares (OLS) regression because it involves more than one explanatory variable.\n",
        "\n",
        "\n",
        "* Multiple linear regression (MLR) is a statistical technique that uses several explanatory variables to predict the outcome of a response variable.\n",
        "\n",
        "* It is also known as multiple regression,\n",
        "Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable.\n",
        "\n",
        "* MLR is used extensively in econometrics and financial inference.\n",
        "\n",
        "* Multiple regressions are used to make forecasts, explain relationships between financial variables, and test existing theories.\n",
        "\n",
        "\n",
        "Que.9  What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Ans Linear regression examines the relationship between one predictor and an outcome, while multiple regression delves into how several predictors influence that outcome. Both are essential tools in predictive analytics, but knowing their differences ensures effective and accurate modelling. Dive in to discover the core distinctions and when to use each approach.\n",
        "\n",
        "Difference Between Linear Regression and Multiple Regression:\n",
        "\n",
        "* Number of Independent Variables: Simple linear regression involves only one independent variable, while multiple linear regression involves two or more independent variables.\n",
        "\n",
        "* Model Complexity: Multiple linear regression is more complex due to the consideration of multiple predictors, leading to a hyperplane rather than a simple line.\n",
        "\n",
        "* Interpretation: In simple linear regression, the slope coefficient indicates the change in the dependent variable for a unit change in the independent variable. In multiple linear regression, the interpretation of the coefficients becomes more nuanced as they represent the change in the dependent variable while holding other variables constant.\n",
        "\n",
        "* Assumptions: Both methods share some assumptions, like linearity, independence of errors, and homoscedasticity (constant variance of errors), but multiple linear regression introduces additional considerations due to the presence of multiple predictors.\n",
        "\n",
        "* Use Cases: Simple linear regression is useful when examining the relationship between two variables, while multiple linear regression is appropriate when multiple predictors are likely to influence the dependent variable.\n",
        "\n",
        " In summary, the primary difference lies in the number of independent variables involved and the resulting complexity of the model. Simple linear regression deals with a single predictor, while multiple linear regression incorporates multiple predictors to capture more intricate relationships.\n",
        "\n",
        "\n",
        " Que 10  What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        " Ans.Multiple linear regression is a statistical method we can use to understand the relationship between multiple predictor variables and a response variable.\n",
        "\n",
        " However, before we perform multiple linear regression, we must first make sure that five assumptions are met:\n",
        "\n",
        "1. Linear relationship: There exists a linear relationship between each predictor variable and the response variable.\n",
        "\n",
        "2. No Multicollinearity: None of the predictor variables are highly correlated with each other.\n",
        "\n",
        "3. Independence: The observations are independent.\n",
        "\n",
        "4. Homoscedasticity: The residuals have constant variance at every point in the linear model.\n",
        "\n",
        "5. Multivariate Normality: The residuals of the model are normally distributed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Que 11  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Ans.Heteroscedasticity, sometimes spelled heteroskedasticity, refers to the unequal scatter of residuals or error terms in regression analysis. Specifically, it indicates a systematic change in the spread of the residuals over the range of measured values\n",
        "\n",
        "This phenomenon violates one of the key assumptions of ordinary least squares (OLS) regression, which assumes that the residuals have constant variance, known as homoscedasticity\n",
        "\n",
        "Effects on Regression Analysis:\n",
        "\n",
        "When heteroscedasticity is present, it increases the variance of the regression coefficient estimates, making the results of the analysis unreliable. This can lead to incorrect conclusions about the statistical significance of the terms in the model\n",
        "\n",
        ". Additionally, the OLS estimators are no longer the Best Linear Unbiased Estimator (BLUE), and hypothesis tests like the t-test and F-test become invalid due to inconsistencies in the covariance matrix of the estimated regression coefficients\n",
        "\n",
        "\n",
        "Que.12  How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "Ans. Multicollinearity occurs when two or more independent variables in a linear regression model are highly correlated. To address multicollinearity, here are a few simple strategies:\n",
        "\n",
        " * Increase the sample size: to improve model accuracy, making it easier to differentiate between the effects of different predictors.\n",
        "\n",
        "* Remove highly correlated predictors using Variance Inflation Factor (VIF), which tells you if certain variables are highly correlated. If the VIF is too high, consider removing one of the correlated predictors to improve model stability.\n",
        "\n",
        "* Combine correlated variables and combine them into a single, more meaningful predictor. This can be done using techniques like Principal Component Analysis (PCA) or factor analysis, which help reduce redundancy by creating a new variable that represents the combined information.\n",
        "\n",
        "Que 13  What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "Ans. Categorical variables are non-numeric variables that represent groups or categories. In regression models, which typically require numeric inputs, handling categorical variables appropriately is crucial for building accurate and interpretable models.\n",
        "\n",
        "common techniques:\n",
        "\n",
        "* One-Hot Encoding: Convert categorical variables into binary columns, where each column corresponds to a unique category of the variable.\n",
        "\n",
        "* Regression: Once the categorical variables are encoded, they can be used as features (independent variables) in a regression model.\n",
        "\n",
        "* Fit a Linear Regression Model: Use the encoded features along with a target variable to fit a linear regression model.\n",
        "\n",
        "Que.14 What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        "Ans Considering interactions in multiple linear regression is crucial for gaining a fuller understanding of the relationships between predictors and preventing misleading interpretations.\n",
        "\n",
        "Que 15  How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "Ans.The intercept (sometimes called the “constant”) in a regression model represents the mean value of the response variable when all of the predictor variables in the model are equal to zero.\n",
        "\n",
        "Interpreting the Intercept in Simple Linear Regression\n",
        "A simple linear regression model takes the following form\n",
        "\n",
        "ŷ = β0 + β1(x)\n",
        "\n",
        "where:\n",
        "\n",
        "ŷ: The predicted value for the response variable\n",
        "β0: The mean value of the response variable when x = 0\n",
        "β1: The average change in the response variable for a one unit increase in x\n",
        "x: The value for the predictor variable\n",
        "\n",
        "\n",
        "Interpreting the Intercept in Multiple Linear Regression\n",
        "\n",
        "A multiple linear regression model takes the following form:\n",
        "\n",
        "ŷ = β0 + β1(x1) + β2(x2) + β3(x3) + … + βk(xk)\n",
        "\n",
        "where:\n",
        "\n",
        "ŷ: The predicted value for the response variable\n",
        "β0: The mean value of the response variable when all predictor variables are zero\n",
        "βj: The average change in the response variable for a one unit increase in the jth predictor variable, assuming all other predictor variables are held constant\n",
        "xj: The value for the jth predictor variable\n",
        "\n",
        "Que.16  What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "Ans The significance of a regression slope lies in its ability to provide valuable insights into the strength and direction of the relationship between the variables. It helps in understanding the impact of one variable on another and can be used to make predictions and draw conclusions about the data.\n",
        "\n",
        "A regression slope is a measure of the relationship between two variables in a linear regression model. It represents the change in the dependent variable for a unit change in the independent variable. The significance of a regression slope lies in its ability to provide valuable insights into the strength and direction of the relationship between the variables. It helps in understanding the impact of one variable on another and can be used to make predictions and draw conclusions about the data. The slope also plays a crucial role in determining the statistical significance of the relationship between the variables, allowing researchers to make informed decisions based on the data analysis. In summary, the regression slope is a fundamental statistic in regression analysis that helps in understanding and interpreting the relationship between variables.\n",
        "\n",
        "\n",
        "\n",
        "Que 17  How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "Ans.The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. The slope and the intercept define the linear relationship between two variables, and can be used to estimate an average rate of change.\n",
        "\n",
        "\n",
        "Que.18 What are the limitations of using R² as a sole measure of model performance\n",
        "\n",
        "Ans.R-squared (R²) is a statistical metric used in machine learning to assess the performance of regression models. It measures how well the model’s predictions fit the actual data, indicating the proportion of variance in the dependent variable that is explained by the independent variables. The R² value ranges from 0 to 1, with 1 representing a perfect fit, meaning the model accounts for all variance, and 0 indicating that the model explains none of the variance.\n",
        "\n",
        "1. Overfitting Risk\n",
        "One of the major limitations of R² is its tendency to increase with the addition of more predictors, regardless of whether those predictors improve the model's ability to generalize. This can lead to overfitting, where the model becomes excessively complex and closely fits the training data, including noise and random fluctuations.\n",
        "\n",
        "As a result, while R² may appear high, the model might not perform well on new, unseen data. This makes R² less reliable for models with many predictors, as it may give a false sense of accuracy.\n",
        "\n",
        "2. Not Suitable for Non-linear Models\n",
        "R² assumes a linear relationship between the independent and dependent variables, which limits its effectiveness in assessing non-linear models.\n",
        "\n",
        "When the relationship between variables is not linear, R² can mislead users into believing that the model fits the data well, even if it doesn’t capture the true underlying patterns. Non-linear models often require alternative metrics or specialized methods to evaluate their performance accurately.\n",
        "\n",
        "3. Doesn’t Indicate Model Bias\n",
        "R² does not provide any insight into model bias, meaning it doesn't tell you whether the model is under-predicting or over-predicting the target variable. A high R² might still coincide with a systematically biased model, producing predictions that are consistently too high or too low.\n",
        "\n",
        "\n",
        "This limitation means that R² should be used in conjunction with other metrics, such as residual analysis, to evaluate model accuracy and avoid potential issues with biased predictions.\n",
        "\n",
        "\n",
        "4. Ignores Model Complexity\n",
        "R² can give misleading results when comparing models with different numbers of predictors. Since R² always increases or stays the same as more predictors are added to the model, it does not penalize unnecessary or redundant features.\n",
        "\n",
        "\n",
        "This can make complex models with irrelevant features appear to perform better than simpler, more focused models. To address this issue, Adjusted R² or other complexity-aware metrics are often recommended when comparing models with varying numbers of predictors.\n",
        "\n",
        "5. Sensitive to Outliers\n",
        "Another limitation of R² is its sensitivity to outliers. A few extreme values can significantly distort the R² score, either inflating it or making it appear lower than it should be. In the presence of outliers, R² may give an inaccurate picture of the model's true performance.\n",
        "\n",
        "This can lead to incorrect conclusions about how well the model fits the data. To mitigate this, alternative metrics or outlier detection methods should be considered alongside R².\n",
        "\n",
        "\n",
        "6. Limited for Small Datasets\n",
        "R² may not be a reliable metric when working with small datasets. With fewer data points, the R² value can fluctuate significantly, leading to unstable or misleading results. In such cases, the model's performance may seem much worse or better than it actually is.\n",
        "\n",
        "\n",
        "As the sample size increases, R² tends to become more stable and reflective of the model’s true performance. For small datasets, other evaluation techniques, such as cross-validation, may provide a more accurate picture.\n",
        "\n",
        "7. Does Not Measure Prediction Accuracy\n",
        "While R² is a useful indicator of how well the model fits the training data, it does not directly measure the model's ability to make accurate predictions on new, unseen data. A model with a high R² could still have poor predictive accuracy, especially if it is overfitting to the training data.\n",
        "\n",
        "Therefore, R² should not be the sole criterion for model evaluation. Additional metrics like Mean Squared Error (MSE) or cross-validation performance should be considered to evaluate the model's true predictive power.\n",
        "\n",
        "\n",
        "Que.19  How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "Ans.The standard error of the regression (S) is a crucial metric in linear regression analysis. It measures the average distance that the observed values fall from the regression line. This metric is particularly useful for assessing the precision of predictions made by the regression model.\n",
        "\n",
        "Key Concepts\n",
        "\n",
        "Standard Error vs. R-Squared: While R-squared (R²) indicates the proportion of variance in the response variable explained by the predictor variable, the standard error provides a more tangible measure of prediction accuracy. For instance, if the standard error is 4.89, it means that the observed values fall an average of 4.89 units from the regression line.\n",
        "\n",
        "Precision of Predictions: The standard error can be used to approximate a 95% prediction interval. Roughly 95% of the observations should fall within +/- two standard errors of the regression. This gives a quick estimate of how precise the predictions will be.\n",
        "\n",
        "Units of Measurement: Unlike R-squared, the standard error is expressed in the same units as the response variable, making it easier to interpret.\n",
        "\n",
        "Prediction Intervals: For making predictions, the standard error provides a clear indication of the precision. For example, a model with a standard error of 2.095 will have a 95% prediction interval of +/- 4.19 units, which is more precise than a model with a standard error of 4.19\n",
        "\n",
        "\n",
        "Que.20  How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\n",
        "Ans. Prerequisite: Linear Regression In Simple Linear Regression or Multiple Linear Regression we make some basic assumptions on the error term\\epsilon .\n",
        "\n",
        " Simple Linear Regression:\n",
        "\n",
        " Yi = beta0 + \\beta1 Xi +ei\n",
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        " Yi = beta0 + beta1 Xi1 + beta2 Xi2 + .... + beta n X in +ei\n",
        "\n",
        "Assumptions:\n",
        "1. Error has zero mean\n",
        "2. Error has constant variance\n",
        "3. Errors are uncorrelated\n",
        "4. Errors are normally distributed\n",
        "The second assumption is known as Homoscedasticity and therefore, the violation of this assumption is known as Heteroscedasticity.\n",
        "\n",
        "Therefore, in simple terms, we can define heteroscedasticity as the condition in which the variance of error term or the residual term in a regression model varies. As you can see in the above diagram, in case of homoscedasticity, the data points are equally scattered.\n",
        "\n",
        "Possible reasons of arising Heteroscedasticity:\n",
        "\n",
        "\n",
        "* Often occurs in those data sets which have a large range between the largest and the smallest observed values i.e. when there are outliers.\n",
        "\n",
        "* When model is not correctly specified.\n",
        "\n",
        "* If observations are mixed with different measures of scale.\n",
        "\n",
        "* When incorrect transformation of data is used to perform the regression.\n",
        "\n",
        "* Skewness in the distribution of a regressor, and may be some other sources.\n",
        "\n",
        "\n",
        "Que.21  What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        "\n",
        "\n",
        "Ans. A high R² but low adjusted R² indicates that the model may be overfitted and that some of the independent variables may not be contributing to the model. Adjusted R² accounts for the number of predictors in the model and penalizes for adding irrelevant predictors.  \n",
        "     \n",
        "     Example: If R² is 0.9 but adjusted R² is 0.6, it means that the model may be overfitted and that some of the predictors may not be contributing to the model.  \n",
        "     \n",
        "\n",
        " Que.22  Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        " Ans. It is important to scale variables in Multiple Linear Regression to ensure that all variables are on the same scale and to prevent any one variable from dominating the model. Scaling also helps to improve the convergence of optimization algorithms.  \n",
        "\n",
        "   Example:  If one variable is measured in dollars and another in thousands of dollars, scaling ensures that both variables are on the same scale.  \n",
        "\n",
        "\n",
        "Que.23  What is polynomial regression\n",
        "\n",
        "Ans.Polynomial regression is a type of regression analysis used in statistics and machine learning to model the relationship between a dependent variable ( y ) and an independent variable ( x ) as an ( n )-degree polynomial.\n",
        "\n",
        "Que.24  How does polynomial regression differ from linear regression\n",
        "\n",
        "Ans.Polynomial regression differs from linear regression in that it models the relationship between the variables as a polynomial, rather than a straight line. This allows polynomial regression to capture non-linear relationships between the variables.  \n",
        "\n",
        "Both linear and polynomial regression have their places in predictive modeling. Linear regression is simpler and works well for linear relationships, while polynomial regression is more flexible and can model more complex relationships.\n",
        "\n",
        "\n",
        "Que.25  When is polynomial regression used\n",
        "\n",
        "Ans. Polynomial regression is used when the relationship between the independent and dependent variables is non-linear and cannot be adequately captured by a straight line. It is also used when the residuals show a pattern that suggests a non-linear relationship.  \n",
        "\n",
        "If the residuals of a linear regression model show a curved pattern, it indicates that a polynomial regression model may be more appropriate.  \n",
        "\n",
        "Que.26  What is the general equation for polynomial regression\n",
        "\n",
        "Ans.The general equation for polynomial regression is:\n",
        "\n",
        "      `Y = b0 + b1X + b2X² + ... + bnXⁿ`\n",
        "\n",
        "     \"  where Y is the dependent variable, X is the independent variable, and b0, b1, ..., bn are the coefficients.  \n",
        "\n",
        "Que.27  Can polynomial regression be applied to multiple variables\n",
        "\n",
        "Ans. Yes, polynomial regression can be applied to multiple variables. This is known as multiple polynomial regression, where the relationship between the dependent variable and multiple independent variables is modeled as a polynomial.\n",
        "\n",
        "Que.28 What are the limitations of polynomial regression\n",
        "\n",
        "Ans. Limitations of polynomial regression:\n",
        "\n",
        "\n",
        "* Overfitting:  If we include too many polynomial features (i.e., increase the degree of the polynomial), our model can become too complex. It might fit the training data almost perfectly, but perform poorly on new, unseen data. This is because it’s not only modeling the underlying trend but also the noise in the data. This is called overfitting.\n",
        "\n",
        "* Underfitting:   On the other hand, if we don’t include enough polynomial features (i.e., the degree of the polynomial is too low), our model may be too simple to capture the underlying trend. It might perform poorly on both the training data and new data. This is called underfitting.\n",
        "\n",
        "\n",
        "* Choice of Degree: Choosing the right degree for the polynomial can be challenging. It’s usually done by trial and error, testing different degrees and comparing their performance. However, there is no one-size-fits-all solution – the optimal degree may differ for different datasets and problem contexts.\n",
        "\n",
        "* Computational Complexity: Polynomial Regression involves more computations than simple Linear Regression, as it needs to calculate powers and interaction terms. This might make it less suitable for very large datasets.\n",
        "\n",
        "\n",
        "Que.29  What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\n",
        "Ans.  Methods to evaluate model fit include cross-validation, adjusted R², and visual inspection of residual plots. Cross-validation helps to assess the model's performance on unseen data, while adjusted R² accounts for the number of predictors in the model. Residual plots help to identify patterns in the residuals.  \n",
        "\n",
        "\n",
        "Que.30  Why is visualization important in polynomial regression\n",
        "\n",
        "Ans.Visualization is important in polynomial regression because it helps to understand the relationship between the variables, identify patterns in the residuals, and assess the model fit. Visualization also helps to communicate the results of the analysis to others.\n",
        "\n",
        "\n",
        "\n",
        "Que.31  How is polynomial regression implemented in Python\n",
        "\n",
        "Ans.Polynomial regression can be implemented in Python using libraries such as NumPy, SciPy, and scikit-learn. The process involves creating polynomial features, fitting a linear regression model to the transformed features, and making predictions.  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YTl7DKEZa-Fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZYwW5Ywqkivh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bk9ZpDd1ekEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sXOEHhIpek4x"
      }
    }
  ]
}